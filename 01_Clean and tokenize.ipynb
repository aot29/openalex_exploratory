{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da503e3b-4766-4c39-a7b5-3cc59a1cfed1",
   "metadata": {},
   "source": [
    "# Clean and tokenize\n",
    "The dataset obtained from OpenAlex (see README.md) was reviewed using on HubMeta. \n",
    "* Load the metadata of the selected 682 papers as a data frame\n",
    "* Split data into train/validate/test\n",
    "* Aplly pre-processing filters\n",
    "* Apply lemmatization\n",
    "* Save tokenized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6b48d6-6af2-47e3-b9dc-b8b106f26100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tok\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "import os\n",
    "from RISparser import readris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7cc34b-e94b-41ad-806d-b170d16f1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaab500-7037-4262-8f05-2407d8079bb9",
   "metadata": {},
   "source": [
    "## Load the metadata of the selected 682 papers as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a198747d-b8b3-4218-82dc-3fc8984bca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filepath = os.path.join(DATA_PATH, 'export_paper_2024-10-29_17-23-53.ris')\n",
    "with open(filepath, 'r') as bibliography_file:\n",
    "    entries = readris(bibliography_file)\n",
    "bib_df_sdp = pd.DataFrame(list(entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7796b1-005b-4c74-b4c8-5cfbec4fb8e5",
   "metadata": {},
   "source": [
    "## Split the data into train / validate / test datasets\n",
    "\n",
    "\"train\"\n",
    "\n",
    "    A percent of the texts reserved for fitting the model.\n",
    "\n",
    "\"validate\"\n",
    "\n",
    "    A percent of the texts reserved for computing perplexity when fitting the model's k-parameter, and searching for best parameters.\n",
    "\n",
    "\"test\"\n",
    "\n",
    "    A percent of the texts reserved for testing hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a9da60-38c7-41e2-884a-b91596233b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    train, test = train_test_split(df, test_size=0.5)\n",
    "    validate, test = train_test_split(test, test_size=0.5)\n",
    "    return(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45552b1c-8e5d-4522-88a5-83708512e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset for SDP has 341 rows, the validate dataset 170 rows, the test dataset 171 rows\n"
     ]
    }
   ],
   "source": [
    "# split papers on Scholarly Document Processing (sdp)\n",
    "train_sdp, validate_sdp, test_sdp = split(bib_df_sdp)\n",
    "print(f\"The train dataset for SDP has {train_sdp.shape[0]} rows, the validate dataset {validate_sdp.shape[0]} rows, the test dataset {test_sdp.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f980c-e99e-46b8-9351-0b457728b127",
   "metadata": {},
   "source": [
    "## Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69677137-6de7-4dd2-ab7e-16396844bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(filename, path, df):\n",
    "    with zf.ZipFile(path, 'w') as ziparchive:\n",
    "        ziparchive.writestr(filename, df.to_csv())\n",
    "\n",
    "def save_datasets(name, train, validate, test):\n",
    "    filename_train = f\"{name}_train.csv\"\n",
    "    path_train = os.path.join(DATA_PATH, f\"{filename_train}.zip\")\n",
    "    save_dataset(filename_train, path_train, train)\n",
    "    \n",
    "    filename_validate = f\"{name}_validate.csv\"\n",
    "    path_validate = os.path.join(DATA_PATH, f\"{filename_validate}.zip\")\n",
    "    save_dataset(filename_validate, path_validate, validate)\n",
    "\n",
    "    filename_test = f\"{name}_test.csv\"\n",
    "    path_test = os.path.join(DATA_PATH, f\"{filename_test}.zip\")\n",
    "    save_dataset(filename_test, path_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4570c64-fd9e-463f-8126-6cdd1f25c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data on Scholarly Document Processing\n",
    "save_datasets('sdp', train_sdp, validate_sdp, test_sdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257de222-a727-42fd-a16d-7c9cf2fee952",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Apply pre-processing filters: strip_tags, strip_punctuation, strip_multiple_whitespaces, stric_numeric, remove_stopwords; strip_short\n",
    "\n",
    "Apply lemmatization to the list of words.\n",
    "\n",
    "The tokenize functions are in the tok.py package provided in the same directory as the notebooks.\n",
    "\n",
    "see: https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a53213fc-5763-44f7-8f99-6d87bac1d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with all the words in the complete SDP dataset\n",
    "texts_sdp = tok.clean(bib_df_sdp['abstract'])\n",
    "dictionary_sdp = tok.make_dictionary(texts_sdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b97ef6-e33e-4f73-91fe-0249c395bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dictionary, df):\n",
    "    _texts = tok.clean(df['abstract'])\n",
    "    return(tok.make_corpus(dictionary, _texts))\n",
    "\n",
    "def tokenize_datasets(dictionary, train, validate, test):\n",
    "    corpus_train = tokenize_dataset(dictionary, train)\n",
    "    corpus_validate = tokenize_dataset(dictionary, validate)\n",
    "    corpus_test = tokenize_dataset(dictionary, test)\n",
    "    return(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41a03540-f0f9-4450-b47b-41bfb73308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data on Scholarly Document Processing\n",
    "corpus_train_sdp, corpus_validate_sdp, corpus_test_sdp = tokenize_datasets(dictionary_sdp, train_sdp, validate_sdp, test_sdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c753fd-5766-4f7f-9ed9-7dd39d747cc9",
   "metadata": {},
   "source": [
    "## Save tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "299f7623-b135-4e44-a345-81e6329e0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary_sdp.pickle'), 'wb') as handle:\n",
    "    pickle.dump(dictionary_sdp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccf36f96-5927-474f-a400-023137e75cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenized_dataset(path, obj):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_tokenized_datasets(name, train, validate, test):\n",
    "    path_train = os.path.join(DATA_PATH, f\"corpus_train_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_train, train)\n",
    "\n",
    "    path_validate = os.path.join(DATA_PATH, f\"corpus_validate_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_validate, validate)\n",
    "\n",
    "    path_test = os.path.join(DATA_PATH, f\"corpus_test_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c11075b2-1c8c-4c1a-9c32-a3c5c5a1c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data on Scholarly Document Processing\n",
    "save_tokenized_datasets('sdp', corpus_train_sdp, corpus_validate_sdp, corpus_test_sdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec671f-2e48-44f2-8267-bf71c37e4fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
